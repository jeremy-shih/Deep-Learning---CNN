{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Deep Learning - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (AUTHOR : JEREMY SHIH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    Loads data from MNIST-yann-lecun.\n",
    "    Input:\n",
    "        -None\n",
    "    Output:\n",
    "        - train_data\n",
    "        - train_labels\n",
    "        - test_data\n",
    "        - test_labels\n",
    "    '''\n",
    "    # read in training and testing data from MNIST-yann-lecun\n",
    "    train_data = idx2numpy.convert_from_file('MNIST/train-images-idx3-ubyte') \n",
    "    train_labels = idx2numpy.convert_from_file('MNIST/train-labels-idx1-ubyte') # (60000,)\n",
    "    test_data = idx2numpy.convert_from_file('MNIST/t10k-images-idx3-ubyte') \n",
    "    test_labels = idx2numpy.convert_from_file('MNIST/t10k-labels-idx1-ubyte') # (10000,)\n",
    "\n",
    "    # reshape dataset to have a single channel\n",
    "    train_data = train_data.reshape((train_data.shape[0], 28, 28, 1)) # (60000, 28, 28, 1)\n",
    "    test_data = test_data.reshape((test_data.shape[0], 28, 28, 1)) # (10000, 28, 28, 1)\n",
    "\n",
    "    # one hot encode target values\n",
    "    train_labels = to_categorical(train_labels)\n",
    "    test_labels = to_categorical(test_labels)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pixels(train, test):\n",
    "    '''\n",
    "    Scale pixels to 0-1 (normalization)\n",
    "    \n",
    "    Input:\n",
    "        - train\n",
    "        - test\n",
    "    Output:\n",
    "        - train_norm\n",
    "        - test_norm\n",
    "        \n",
    "    Note: Normalization reduces the complexity of the problem your network is trying to solve\n",
    "    can increase the accuracy of yiour model and speed up the training.\n",
    "    For example, if you give two features equal importance -> ex: age between 0-120, \n",
    "    but income between 10k-100k -> initially income more important than age (weighted more).\n",
    "    '''\n",
    "    # integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    \n",
    "    return train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def define_model():\n",
    "    '''\n",
    "    Define a CNN (sequential model).\n",
    "    \n",
    "    Input:\n",
    "        - None\n",
    "    Output:\n",
    "        - CNN model\n",
    "    \n",
    "    Conv2D:\n",
    "        - filters = dimentionality of the output\n",
    "        - kernel size = height and width of 2D convolution window\n",
    "        - activation function\n",
    "        - kernel_initializer = initializer for the kernel weights matrix\n",
    "        - input shape -> when using this as the first layer in a model\n",
    "        \n",
    "    MaxPooling2D\n",
    "        - pool_size = window size over which to take the maximum\n",
    "        \n",
    "    Flatten -> flattens the input, does not affect the batch size\n",
    "    \n",
    "    Dense -> implements 'output = activation(dot(input, kernel) + bias)'\n",
    "        - units = dimensionality of the output space\n",
    "        - activation_Function = activation function to use\n",
    "        - kernel_initializer = regularizer function applied to the kernel weights matrix\n",
    "            -> 'he_uniform' -> draws samples from a uniform distribution\n",
    "            \n",
    "    SGD (Stochastic Gradient Descent Optimizer)\n",
    "        - learning rate\n",
    "        - momentum -> accelerates gradient descent in the relevant direction and dampens oscillations\n",
    "    \n",
    "    Compile\n",
    "        - optimizer -> name of optimizer\n",
    "        - loss -> objective function\n",
    "        - metrics -> list of metrics displayed during fit() and logged to the History object returned by fit()\n",
    "            -> evaluated by the model during training and testing\n",
    "    \n",
    "    Note: A sequential model is appropriate for a plain stack of layers where each layer\n",
    "    has exactly one input tensor and one output sensor.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    # FRONT END - convolutional and pooling layers\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1))) # filter size of (3,3) and number of filters = 32\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Flatten()) # flatten to provide features to the classifier -> convert to vector\n",
    "    model.add(Dropout(0.5)) # prevent overfitting\n",
    "    \n",
    "    # BACK END\n",
    "    # add dense layer to interpret features\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform')) \n",
    "    model.add(Dense(10, activation='softmax')) # require 10 nodes to predict 10 classes\n",
    "    \n",
    "    # OPTIMIZER USED\n",
    "    opt = SGD(lr=0.01, momentum=0.9) # Stochastic gradient descent optimizer: learning rate=0.01 and momentum=0.9\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) # categorical cross-entropy loss function will be optimized -> suitable for multi-class classification\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               540900    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 542,230\n",
      "Trainable params: 542,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Classification accuracy for training set is:  0.9976500272750854\n",
      "Classification accuracy for testing set is:  0.9907000064849854\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data, train_labels, test_data, test_labels = load_data()\n",
    "\n",
    "# Prep Pixels (normalize)\n",
    "train_data, test_data = prep_pixels(train_data, test_data)\n",
    "\n",
    "# Define Model\n",
    "model = define_model()\n",
    "\n",
    "# Print model summary and save model_plot image\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "# TRAINS THE MODEL ON TRAINED CNN (USING TRAINING MNIST AS INPUT)\n",
    "'''\n",
    "-input data\n",
    "-target data\n",
    "- batch size = number of samples per gradient update\n",
    "- epochs = number of iterations over entire x and y data provided\n",
    "- verbose = 0 = silent\n",
    "- validation data = data which to evluate the loss and any model metrics at the end of each epoch\n",
    "'''\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(train_data, train_labels), verbose=0)\n",
    "\n",
    "\n",
    "# USE TRAINING MNIST AS INPUT TO TRAINED CNN\n",
    "'''\n",
    "input_data, target data, verbose = 0 = silent\n",
    "'''\n",
    "loss_value, accuracy = model.evaluate(train_data, train_labels, verbose=0) \n",
    "'''\n",
    "loss value = how well or poorly a certain model behaves after each iteration of optimization\n",
    "the lower the loss, the better the omodel -> calculated on training and validation\n",
    "interpretation is how well the model is doing for these two sets\n",
    "loss is not a percentage, but a summation of the errors made for each example in training or validation sets\n",
    "'''\n",
    "print(\"Classification accuracy for training set is: \", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINS THE MODEL ON TRAINED CNN (USING TESTING MNIST AS INPUT)\n",
    "'''\n",
    "-input data\n",
    "-target data\n",
    "- batch size = number of samples per gradient update\n",
    "- epochs = number of iterations over entire x and y data provided\n",
    "- verbose = 0 = silent\n",
    "- validation data = data which to evluate the loss and any model metrics at the end of each epoch\n",
    "'''\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels), verbose=0)\n",
    "\n",
    "\n",
    "# USE TRAINING MNIST AS INPUT TO TRAINED CNN\n",
    "'''\n",
    "input_data, target data, verbose = 0 = silent\n",
    "'''\n",
    "loss_value, accuracy = model.evaluate(test_data, test_labels, verbose=0) \n",
    "'''\n",
    "loss value = how well or poorly a certain model behaves after each iteration of optimization\n",
    "the lower the loss, the better the omodel -> calculated on training and validation\n",
    "interpretation is how well the model is doing for these two sets\n",
    "loss is not a percentage, but a summation of the errors made for each example in training or validation sets\n",
    "'''\n",
    "print(\"Classification accuracy for testing set is: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
